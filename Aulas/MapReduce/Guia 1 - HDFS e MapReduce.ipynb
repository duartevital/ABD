{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab #1 - HDFS e MapReduce\n",
    "\n",
    "This lab demonstrates the use of HDFS and Hadoop's Map Reduce algorithm. We will use data from the National Climatic Data Center (NCDC), available at http://www.ncdc.noaa.gov/. This data is stored using a line-oriented ASCII format, one record each line. The format supports a rich set of meteorological elements, many of which are optional or with variable data lengths. The following lines are a sample of one of those files:\n",
    "\n",
    "    0067011990999991950051507004+68750+023550FM-12+038299999V0203301N00671220001CN9999999N9+00001+99999999999\n",
    "    0043011990999991950051512004+68750+023550FM-12+038299999V0203201N00671220001CN9999999N9+00221+99999999999\n",
    "    0043011990999991950051518004+68750+023550FM-12+038299999V0203201N00261220001CN9999999N9-00111+99999999999\n",
    "    0043012650999991949032412004+62300+010750FM-12+048599999V0202701N00461220001CN0500001N9+01111+99999999999\n",
    "    0043012650999991949032418004+62300+010750FM-12+048599999V0202701N00461220001CN0500001N9+00781+99999999999\n",
    "\n",
    "\n",
    "The following lines explain what is the meaning of the fields (note that fields are packed into one line with no delimiters):\n",
    "\n",
    "    0067\n",
    "    011990   # USAF weather station identifier\n",
    "    99999    # WBAN weather station identifier\n",
    "    19500515 # observation date\n",
    "    0700     # observation time\n",
    "    4\n",
    "    +68750   # latitude (degrees x 1000)\n",
    "    +023550  # longitude (degrees x 1000)\n",
    "    FM-12\n",
    "    +0382    # elevation (meters)\n",
    "    99999\n",
    "    V020\n",
    "    330      # wind direction (degrees)\n",
    "    1        # quality code\n",
    "    N\n",
    "    0067\n",
    "    1\n",
    "    22000    # sky ceiling height (meters)\n",
    "    1        # quality code\n",
    "    C\n",
    "    N\n",
    "    999999   # visibility distance (meters)\n",
    "    9        # quality code\n",
    "    N\n",
    "    9\n",
    "    +0000    # air temperature (degrees Celsius x 10)\n",
    "    1        # quality code\n",
    "    +9999    # dew point temperature (degrees Celsius x 10)\n",
    "    9        # quality code\n",
    "    99999    # atmospheric pressure (hectopascals x 10)\n",
    "    9        # quality code\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subset of the dataset was downloaded from the server. It compreends datafiles from 1901 to 1960. To avoid to many small files, records were grouped by year resulting in a bigger file.\n",
    "\n",
    "For simplicity, we will focus on the temperature, which is always present and is of fixed width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we going to solve?\n",
    "\n",
    "What we are going to find is maximum temperature registered of each year, using Map Reduce. Documentation is available on hadoop's website: https://hadoop.apache.org/docs/r1.2.1/streaming.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our setup, some environment variables are not automatically loaded, in particular, the path to hadoop's executables. \n",
    "\n",
    "So let's add it for jupyter notebook as well as for the terminal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = environment['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PATH=$PATH:/usr/local/hadoop/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HADOOP_HOME=/usr/local/hadoop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminal\n",
    "\n",
    "In ther terminal, run the following command:\n",
    "\n",
    "    source /etc/profile.d/apache-hadoop.sh\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore HDFS file system\n",
    "\n",
    "The HDFS has a web page where we can chech the status, the architecture as well as browse the file system. The web page is available at https://iscte.me/hdfs .\n",
    "\n",
    "There, however, some commands to work with HDFS. Let's see some of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files of your home\n",
    "\n",
    "Note: `%%bash` is magic the will interpret all the following commands of the cell as bash commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets directory\n",
    "\n",
    "Some datasets are already in our system. They are available at `/home/ABD/datasets`\n",
    "\n",
    "Note: the magic `%%script` allows to interpret the following commands with any kind of enviroment (bash, python, sh, zsh, etc...). \n",
    "\n",
    "Thus, `%%script env bash` is equivalent to `%%bash`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script env bash \n",
    "\n",
    "hdfs dfs -ls /home/ABD/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the size of each dataset?\n",
    "\n",
    "Enter a command bellow to see each dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the sample file from HDFS to a local folder.\n",
    "\n",
    "\n",
    "Copy the sample file to a local folder. The sample is in the HDFS in the following path:\n",
    "\n",
    "    /home/ABD/datasets/ncdc-sample/sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the sample file's content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Map Reduce algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper function\n",
    "\n",
    "Use the magic `%%file` to create a mapper file. The year is available at positions 15 until 19, the temperatura fom 87 to 92, and the quality value from 92 to 93. \n",
    "\n",
    "Process each line in a for loop through the standard input, i.e., sys.stdin. Strip the result, extract the info and check if the temperature is not \"+9999\" and the quality value belongs to {0, 1, 4, 5, 9}. It both conditions are met, print key value pair, separated by a tab.\n",
    "\n",
    "To check if a number belongs to a set can be done using regular expressions: `re.match([01459], variable)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file mapper.py\n",
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # your code here\n",
    "    val = line.strip()\n",
    "    (year, temp, q) = (val[15:19], val[87:92], val[92:93])\n",
    "    if temp != \"+9999\" and re.match(\"[01459]\", q):    \n",
    "        print(\"{}\\t{}\".format(year, temp))\n",
    "\n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the mapper function\n",
    "\n",
    "To test your mapper function, run the following command:\n",
    "\n",
    "    cat sample.txt | /home/.../mapper.py\n",
    "    \n",
    "Note: enter the full path to your function mapper.py\n",
    "\n",
    "Expected values:\n",
    "\n",
    "    1950\t+0000\n",
    "    1950\t+0022\n",
    "    1950\t-0011\n",
    "    1949\t+0111\n",
    "    1949\t+0078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort\n",
    "\n",
    "Sort can be achieved by using the `sort` command:\n",
    "\n",
    "    cat sample.txt | /home/.../mapper.py | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer\n",
    "\n",
    "The reducer receives a lot of temperatures from the same year, and outputs the maximum value. The funcion is already defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file reducer.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "last_key, max_val = (None, -sys.maxsize)\n",
    "for line in sys.stdin:\n",
    "    key, val = line.strip().split('\\t')\n",
    "    if last_key and last_key != key:\n",
    "        print(\"{}\\t{}\".format(last_key, max_val))\n",
    "        last_key, max_val = (key, int(val))\n",
    "    else:\n",
    "        last_key, max_val = key, max(max_val, int(val))\n",
    "\n",
    "if last_key:\n",
    "    print(\"{}\\t{}\".format(last_key, max_val))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test all your functions first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submiting a map reduce job, test all your functions together. We can do so with the following command:\n",
    "\n",
    "    cat sample.txt | /home/joao/hadoop/mapper.py | sort | /home/joao/hadoop/reducer.py \n",
    "    \n",
    "Measure the time it takes to complete using python's module `time`.\n",
    "\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    ...\n",
    "    t1 = time.time()\n",
    "    print('Elapsed time: {}s'.format(t1-t0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# your code here: start with !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Map Reduce job in hadoop\n",
    "\n",
    "The hadoop command to execute our code is the following:\n",
    "\n",
    "    hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar\\\n",
    "       -files mapper.py,reducer.py\\\n",
    "       -input /home/ABD/datasets/ncdc/ \\\n",
    "       -mapper mapper.py\\\n",
    "       -combiner reducer.py\\\n",
    "       -reducer reducer.py\\\n",
    "       -output output\n",
    "\n",
    "The options have the following meaning:\n",
    "\n",
    "- `-files`: the files to upload to all the nodes in hadoop cluster\n",
    "- `-input`: directory where the dataset's files are located\n",
    "- `-mapper`: the mapper function\n",
    "- `-combiner`: the combiner function\n",
    "- `-reducer`: the reducer function\n",
    "- `-output`: the output function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar\\\n",
    "       -files mapper.py,reducer.py\\\n",
    "       -input /home/ABD/datasets/ncdc \\\n",
    "       -mapper mapper.py\\\n",
    "       -combiner reducer.py\\\n",
    "       -reducer reducer.py\\\n",
    "       -output output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the results in the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n",
    "Mapper function: \n",
    "    \n",
    "    %%file mapper.py\n",
    "    #!/usr/bin/env python3\n",
    "    import re\n",
    "    import sys\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        val = line.strip()\n",
    "        (year, temp, q) = (val[15:19], val[87:92], val[92:93])\n",
    "        if temp != \"+9999\" and re.match(\"[01459]\", q):\n",
    "            print(\"{}\\t{}\".format(year, temp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
